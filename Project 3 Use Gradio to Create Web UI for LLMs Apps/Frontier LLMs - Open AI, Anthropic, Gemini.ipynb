{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c145a5eb-4a62-442f-99e3-fe2a18d3ee6f",
   "metadata": {},
   "source": [
    "# Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c74e4fc-6b78-4233-a1da-15f531c0ac0a",
   "metadata": {},
   "source": [
    "Setting up your keys\n",
    "If you haven't done so already, you'll need to create API keys from OpenAI, Anthropic and Google.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/\n",
    "For Anthropic, visit https://console.anthropic.com/\n",
    "For Google, visit https://ai.google.dev/gemini-api\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5cb9225-5c47-4ac3-aef4-fe2dd5077e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "618a082a-cb5b-4825-8740-60ece4f80ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"enter your key\"\n",
    "os.environ['ANTHROPIC_API_KEY'] = \"enter your key\"\n",
    "os.environ['GOOGLE_API_KEY'] = \"enter your key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6def62aa-c379-4812-b128-1d0214a90358",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852dfa9-bd3f-4870-b937-16c4d73dd6e9",
   "metadata": {},
   "source": [
    "# What information is included in the API\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935322f-4fcf-48c1-b985-ea9e43cf9759",
   "metadata": {},
   "source": [
    "Typically we'll pass to the API:\n",
    "\n",
    "1.The name of the model that should be used\n",
    "\n",
    "2.A system message that gives overall context for the role the LLM is playing\n",
    "\n",
    "3.A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including temperature which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85886458-53f1-407d-ab43-b2011478835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58ac7fcd-64fc-42a2-a06a-e66cb6619204",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7992caa4-82eb-4d4c-838a-6ee27d5a489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with their computer? \n",
      "\n",
      "Because it had too many bytes and wasn't their type!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad502ebb-804c-4967-bea7-37d6e61d05eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f859ff2-3e4c-4578-97c9-c7ef3751cb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the project was going to the next level!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58eee4ff-0ad5-4fe9-b7c8-665076298836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd9964-56f3-4c52-a41e-d1df6876f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d32a4218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician? \n",
      "\n",
      "Because they couldn't see eye to eye on the p-value! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9839d982-5f1d-47bf-83af-680c173d1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution?\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9116c859-e341-489d-8dd7-e498bb822ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding whether a business problem is suitable for a Large Language Model (LLM) solution involves evaluating the nature of the problem and the capabilities of LLMs. Here are some key considerations to guide your decision:\n",
       "\n",
       "1. **Nature of the Problem**:\n",
       "   - **Textual Data**: LLMs excel at tasks involving natural language. If your problem involves processing, understanding, or generating text, it might be suitable for an LLM.\n",
       "   - **Complex Language Understanding**: If the problem requires understanding and generating human-like text, such as summarizing documents, generating reports, or answering questions, LLMs could be effective.\n",
       "   - **Conversational Interfaces**: If the task involves building chatbots or virtual assistants that need to interact in a conversational manner, LLMs are well-suited.\n",
       "\n",
       "2. **Task Complexity**:\n",
       "   - **Creative Content Generation**: LLMs can generate creative content, such as writing articles, creating marketing copy, or drafting emails. If the problem involves this kind of task, it could be a good fit.\n",
       "   - **Language Translation and Multilingual Support**: If you need to translate text between languages or provide support in multiple languages, LLMs can be beneficial.\n",
       "\n",
       "3. **Data Availability**:\n",
       "   - **Unstructured Data**: If your data is predominantly unstructured text, such as customer feedback, emails, or social media posts, LLMs can help in extracting insights or generating summaries.\n",
       "   - **Training Data**: Consider whether pre-trained models are sufficient for your needs or if you require fine-tuning on specific domain data for better performance.\n",
       "\n",
       "4. **Scalability and Cost**:\n",
       "   - **Resource Intensity**: LLMs can be resource-intensive. Evaluate whether you have the computational resources and budget to deploy and maintain an LLM solution.\n",
       "   - **Cost vs. Benefit**: Analyze whether the expected benefits from using an LLM outweigh the costs involved in implementing and managing it.\n",
       "\n",
       "5. **Accuracy and Reliability**:\n",
       "   - **Tolerance for Errors**: LLMs might not always produce perfect results. Assess whether your business problem can tolerate occasional errors or if high accuracy is critical.\n",
       "   - **Regulatory and Compliance Needs**: Consider any industry-specific regulations that might affect the use of AI technologies.\n",
       "\n",
       "6. **Integration with Current Systems**:\n",
       "   - **Technical Feasibility**: Evaluate how well an LLM solution can integrate with your existing systems and workflows.\n",
       "   - **Support for APIs and Tools**: Check if there are existing tools and APIs that can facilitate the integration of LLMs with your business processes.\n",
       "\n",
       "7. **Ethical and Privacy Concerns**:\n",
       "   - **Data Privacy**: Consider how using an LLM might impact data privacy and whether it aligns with your organization's privacy policies.\n",
       "   - **Bias and Fairness**: Be aware of potential biases in LLMs and assess the impact they might have on your application.\n",
       "\n",
       "By carefully evaluating these aspects, you can determine if a business problem is suitable for an LLM solution and plan its implementation effectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    \n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b535c6-89e2-4d07-b0f7-662ca903b918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f0cf4-8d4e-45e5-941f-80f5f73d6198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0325cc-3214-42a3-b525-d16c8f1d73e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7299d597-dd85-4588-beb7-7b262e3c362c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c6442-970a-4072-9da5-c2cf060505ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
