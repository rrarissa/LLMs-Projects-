{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7444002-b130-4e2b-9348-91bd0272d0d9",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "In this project, we provide contextual datasets containing internal information about a company, including employee HR details, contracts, products, and company history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10c7d2e-5f80-4939-98ec-b6602d76d1ee",
   "metadata": {},
   "source": [
    "All data provided are fictional and created solely for project purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509607ee-5c7f-4f55-96a6-b91f50e1c652",
   "metadata": {},
   "source": [
    "We will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "932985c5-dc0d-4cc9-a44e-caa0e7ff07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3b1e369d-bc32-4bb7-a8f1-1bf7ac5f9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade chromadb pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7bfd6d03-1db5-4f91-b437-3b0cefedfa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain and Chroma and plotly\n",
    "\n",
    "# Imports from LangChain\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Imports from LangChain's OpenAI embeddings and LLM wrapper\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "# Imports from Chroma and additional libraries\n",
    "from langchain.vectorstores import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Ensure you have the latest versions of langchain, langchain-openai, chromadb, plotly, and sklearn installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb521960-d6a4-4010-b2c6-32e3a665a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0bf393ff-5222-4a4c-9807-58702010e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bf8a760e-c7e4-45b0-b10f-3d24b9fe51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = \"knowledge-base.zip\"\n",
    "\n",
    "# Create a temporary directory to extract the zip contents\n",
    "temp_dir = \"knowledge_base\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(temp_dir)\n",
    "\n",
    "# Load all Markdown files from the extracted directory\n",
    "folders = glob.glob(f\"{temp_dir}/*\")\n",
    "\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4379b52e-b59b-43a7-ad7c-5de53484b587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aa391250-9e82-4d90-9623-c3b2a2920140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6d126541-192f-4262-92df-1c17329ab909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: knowledge-base\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18134b7d-e02b-49eb-ba29-ae2835e101e7",
   "metadata": {},
   "source": [
    "# Vector Database - FAISS (Facebook AI Similarity Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6ecf57d6-bea4-47cd-ba9b-6a9b9fd63ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7f0b6af2-c83c-4fbf-8885-5d2de1fef978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a Chroma Datastore already exists - if so, delete the collection to start from scratch\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fd2b803e-0c32-4d54-b4c0-4b1e3fc3609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 123 vectors with 1,536 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "# Create our Chroma vectorstore!\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "total_vectors = vectorstore.index.ntotal\n",
    "dimensions = vectorstore.index.d\n",
    "\n",
    "print(f\"There are {total_vectors} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bbd10e84-320a-4775-9f46-3e757b5601da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "02f40282-cd7c-4fbe-835b-5bd73290e0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insurellm is an innovative insurance tech startup founded by Avery Lancaster in 2015, aimed at disrupting the insurance industry with advanced products. With 200 employees and 12 offices across the US, Insurellm offers four key software products: Carllm for auto insurance, Homellm for home insurance, Rellm for reinsurance, and Marketllm, a marketplace connecting consumers with insurance providers. The company has rapidly expanded and currently serves over 300 clients worldwide.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you describe Insurellm in a few sentences\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbadae2f-e0ae-4713-aba1-618025ae61cd",
   "metadata": {},
   "source": [
    "# Now we will bring this up in Gradio using the Chat interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1ec019ca-8495-46a1-97bd-315fcf8941e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping that in a function\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ade56f61-2d2f-4bce-9d52-23b12bc9045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/gradio/components/chatbot.py:229: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7884\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7884/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae148c0-bed0-406b-96da-d13346dcbe80",
   "metadata": {},
   "source": [
    "# However, this LLM is not perfect now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2ccfb570-cd47-4f45-a0e5-91235d51c3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "- **2022**: **Satisfactory**  \n",
      "  Avery focused on rebuilding team dynamics and addressing employee concerns, leading to overall improvement despite a saturated market.  \n",
      "\n",
      "- **2023**: **Exceeds Expectations**  \n",
      "  Market leadership was regained with innovative approaches to personalized insurance solutions. Avery is now recognized in industry publications as a leading voice in Insurance Tech innovation.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2020:**  \n",
      "  - Completed onboarding successfully.  \n",
      "  - Met expectations in delivering project milestones.  \n",
      "  - Received positive feedback from the team leads.\n",
      "\n",
      "- **2021:**  \n",
      "  - Achieved a 95% success rate in project delivery timelines.  \n",
      "  - Awarded \"Rising Star\" at the annual company gala for outstanding contributions.  \n",
      "\n",
      "- **2022:**  \n",
      "  - Exceeded goals by optimizing existing backend code, improving system performance by 25%.  \n",
      "  - Conducted training sessions for junior developers, fostering knowledge sharing.  \n",
      "\n",
      "- **2023:**  \n",
      "  - Led a major overhaul of the API internal architecture, enhancing security protocols.  \n",
      "  - Contributed to the companyâ€™s transition to a cloud-based infrastructure.  \n",
      "  - Received an overall performance rating of 4.8/5.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2018**: **3/5** - Adaptable team player but still learning to take initiative.\n",
      "- **2019**: **4/5** - Demonstrated strong problem-solving skills, outstanding contribution on the claims project.\n",
      "- **2020**: **2/5** - Struggled with time management; fell behind on deadlines during a high-traffic release period.\n",
      "- **2021**: **4/5** - Made a significant turnaround with organized work habits and successful project management.\n",
      "- **2022**: **5/5** - Exceptional performance during the \"Innovate\" initiative, showcasing leadership and creativity.\n",
      "- **2023**: **3/5** - Maintaining steady work; expectations for innovation not fully met, leading to discussions about goals.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2023:** Rating: 4.5/5  \n",
      "  *Samuel exceeded expectations, successfully leading a cross-departmental project on AI-driven underwriting processes.*\n",
      "\n",
      "- **2022:** Rating: 3.0/5  \n",
      "  *Some challenges in meeting deadlines and collaboration with the engineering team. Received constructive feedback and participated in a team communication workshop.*\n",
      "\n",
      "- **2021:** Rating: 4.0/5  \n",
      "  *There was notable improvement in performance. Worked to enhance model accuracy, leading to improved risk assessment outcomes for B2C customers.*\n",
      "\n",
      "- **2020:** Rating: 3.5/5  \n",
      "  *Exhibited a solid performance during the initial year as a Senior Data Scientist but had struggles adapting to new leadership expectations.*\n",
      "\n",
      "## Compensation History\n",
      "- **2023:** Base Salary: $115,000 + Bonus: $15,000  \n",
      "  *Annual bonus based on successful project completions and performance metrics.*\n",
      "Human: Who received the prestigious IIOTY award in 2023?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate what gets sent behind the scenes\n",
    "\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "query = \"Who received the prestigious IIOTY award in 2023?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee16211e-0c95-42bf-95e1-b9644505259a",
   "metadata": {},
   "source": [
    "### For a question, the LLM answered it doesn't now. However, it is supporsed to know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f5d0ff14-2a9f-4f37-8429-53d96fbc1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG; \n",
    "# k typically refers to the number of nearest neighbors (or top results) to retrieve based on a given query\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9a543bc9-1830-4988-bcc0-c6ded20c2ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Maxine received the prestigious IIOTY 2023 award.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who received the prestigious IIOTY award in 2023?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2547f6d7-e8d1-4df0-8f93-6591895e2bf3",
   "metadata": {},
   "source": [
    "## This time, the LLM answered the question correctly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
